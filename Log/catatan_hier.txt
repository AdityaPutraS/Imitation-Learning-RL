==================
Eksperimen dibawah telah dihapus, cari di hdd untuk backup
==================
PPO_HumanoidBulletEnv-v0-Hier_84746_00000_0_2021-03-25_11-30-59:
	score high: high target + low target + base reward
	score low: base reward + 0.05 * delta joint + delta end point + low target
	skipframe = 2
	syarat frame update cnt = 5 untuk naikin frame
	syarat delta joint >= 0.5 & delta endpoint >= 0.5
	high obs = cur obs (raw obs dari flat env)
	low obs = curobs[3:3+5] (robot info) + curobs[8:8+17*2] (joint val) + target high level xy 
	calc joint = exp(-5 * dj / weight sum)
	calc endpoint = 2 * exp(-10 * dep / weight sum)
	calc high target = exp(- drobotTarget )
	calc low target = exp(- drobotTargetHL )
	num worker = 5
	num env per worker = 6
	lr = 0.0005
	
	hasil: Belum bisa jalan, robot diam lama, terus jatuh
	
PPO_HumanoidBulletEnv-v0-Hier_2aea3_00000_0_2021-03-25_15-10-23:
	update:
		score high: high target + low target
		score high ketika step remaining <= 0: 0
		skipframe = 1
		weight low agent = [1, 1, 1, 0.5, 0]
		low agent id tetap, tidak berubah ubah sesuai high level step
		syarat delta joint >= 0.15 & delta endpoint >= 0.15
		calc high target = exp(- drobotTarget/5 )
		calc low target = exp(- drobotTargetHL/5 )
		weight joint: 3 -> 1.5 (knee, hip y), sisanya 1
	hasil: Bisa jalan,tp dengan 1 kaki dan masih belum terlalu bisa mengejar target
	
PPO_HumanoidBulletEnv-v0-Hier_ece09_00000_0_2021-03-25_18-50-34:
	update:
		fix incframe: sebelumnya frame = +inc, sekarang frame += inc
		randomize progression di awal, jadi robot berada n% dari target di awal (n = 0 - 80)
		weight low agent = [1, 0.1, 1, 1, 0]
		reward high agent = self.highTargetScore + self.lowTargetScore / (self.steps_remaining_at_level + 1) jika selesai terlebih dahulu dan
		self.highTargetScore + self.lowTargetScore jika selesai saat step sisa 0
		step at level = 100
	global update:
		checkpoint jadi setiap 10 iterasi
		simpan semua checkpoint
		
	hasil: Jatuh terus, terlihat lumayan bisa ke arah target high level, tapi tidak yang ekstrim
	
PPO_HumanoidBulletEnv-v0-Hier_c422a_00000_0_2021-03-25_22-31-20:
	update:
		weight endpoint untuk low level= 1 => 1.5
		weight low target score = 1 => 4
		reward highTarget dan lowTarget dikali 2
		syarat frame update cnt = 5 => 10
		ganti observation low level = targetHL => delta robot dengan target HL
		skipFrame = 2 
		weight joint yang 1.5 jadi 3
	global update:
		model high level = [64, 4] => [16, 8, 4]
		lr = 0.0005 => 0.001
	hasil: Sudah lebih bisa jalan, meskipun jalannya seperti loncat loncat, tapi masih jatuh jatuh
	
PPO_HumanoidBulletEnv-v0-Hier_95788_00000_0_2021-03-26_07-05-25:
	update:
		skipFrame = 2 => 1
		stepPerLevel = 100 => 50
		fix assignment targetHighLevel, sebelumnya hanya action[0], action[1]. Sekarang menjadi vRobot[0]+action[0], vRobot[1]+action[1]
		weight low agent = [1, 0.1, 1.5, 4, 0] => [1, 0.1, 1, 1.5, 0]
		syarat deltaJoints = >=0.2 => >=0.15
		pindah incFrame menjadi setelah assign reward semua
	global update:
		lr = 0.001 => 0.0001
		model high = [16, 8, 4] => [64, 16]
		num env per worker = 6 => 10
	hasil: Menahan postur, berusaha untuk belok, tapi akhirnya berputar putar, gagal mencapai target (baik target akhir / target high level)
	
PPO_HumanoidBulletEnv-v0-Hier_95788_00000_0_2021-03-26_07-05-25:
	update:
		randomX dari -20 s/d 20 => 0 s/d 20
		reward high level agent dari dibagi remaining step menjadi dikali (step_per_level - remaining step + 1)
		weight delta joint menjadi 1
		weight low target score menjadi 1
	global update:
		
	hasil: Gagal
	
==================================================================================
Versi env high baru dengan joint velocity dan observasi yang berbeda
==================================================================================
PPO_HumanoidBulletEnvHier-v0_d61a7_00000_0_2021-04-08_12-35-24:
	update:
		di sesuaikan dengan low level env
		high level mengatur aksi yang harus dilakukan dan mulai dari frame berapa
	global update:
		disesuaikan dengan low level env
		filter observation MeanStdFilter => NoFilter
	hasil: Robot diam di tempat, tidak jatuh (unik sebenarnya karena bisa menahan posisi berdiri)
	
PPO_HumanoidBulletEnvHier-v0_85192_00000_0_2021-04-08_21-37-10:
	update:
		endpoint diubah menjadi koordinat global
		perhitungan score endpoint berdasar delta posisi sendi dan posisi sendi seharusnya menurut dataset
			posisi sendi seharusnya merupakan posisi dataset + posisi ep awal
			posisi ep awal merupakan posisi robot tiap kali high level melakukan step
	global update:
		
	hasil: Gagal
	
PPO_HumanoidBulletEnvHier-v0_d9f3e_00000_0_2021-04-09_08-38-06
PPO_HumanoidBulletEnvHier-v0_ae12f_00000_0_2021-04-09_18-45-20:
	update:
		weight reward endpoint 0.25 => 1
		weight base reward 1 => 0.25
		weight joint endpoint yang 3 => 1.1  yang 1 tetap 1
		assign walk target tiap high level step
	global update:
		model [64, 16] => [512, 128]
		pakai lstm
	hasil: Gagal, di stop karena salah metode target high level
	
PPO_HumanoidBulletEnvHier-v0_f9c67_00000_0_2021-04-09_22-29-21:
	update:
		retrain dengan random target dan yaw
		fix masalah sudut high level target
	global update:
		
	hasil: gagal
	
PPO_HumanoidBulletEnvHier-v0_02a53_00000_0_2021-04-10_06-57-50
PPO_HumanoidBulletEnvHier-v0_48c1b_00000_0_2021-04-10_13-40-40
PPO_HumanoidBulletEnvHier-v0_7bd7f_00000_0_2021-04-11_09-08-54:
	update:
		step per low level dari 50 => sesuai motion yang dipilih high level
	global update:
		ganti sistem resume => restore dari checkpoint tertentu
	hasil: Berjalan tapi hanya 2 langkah,mungkin karena high level masih ngawur dalam memilih
======================================================================
BATAS HAPUS
=====================================================================
PPO_HumanoidBulletEnvHier-v0_66b0b_00000_0_2021-04-11_16-39-16
PPO_HumanoidBulletEnvHier-v0_d6128_00000_0_2021-04-12_06-32-45:
	update:
		high level tidak memilih motion, motion tetap 08_03 saja
		pemilihan target dilakukan dengan memilih sudut acak kemudian x = cos(sudut) * 5, y = sin(sudut) * 5, dibanding
			x = random(-10, 10), y = random(-10, 10)
		step remaining at level kembali menggunakan step per level dibanding menggunakan max frame
		reward weight low level = [0.3, 0.25, 0.25, 1] => [1, 0.25, 0.25, 0.1] (mengikuti low level train)
		high level agent diberi 0.1 * exp dan 0.1 sebagai reward hidupnya
		tambah pengecekan cur timestep di step
	global update:
	
	hasil: Bisa jalan, tapi tidak bisa mencapai target

==> Coba ganti train ke hanya low level saja
======================================================
Policy low level di import dari low level env
note: train harus 2 kali, pertama harus di train minimal 1 iterasi, kemudian di load checkpoint
baru bisa mengimport hasil dari low level
=====================================================
train_HumanoidBulletEnvHier-v0_41476_00000_0_2021-04-19_23-52-33 (BASE)
train_HumanoidBulletEnvHier-v0_03457_00000_0_2021-04-19_23-57-59
train_HumanoidBulletEnvHier-v0_57cd8_00000_0_2021-04-20_00-21-49:
	note:
		reward = [
		    self.delta_deltaJoints,
		    self.delta_deltaVelJoints,
		    self.delta_deltaEndPoints,
		    self.delta_lowTargetScore,
		    self.electricityScore,
		    self.jointLimitScore,
		    self.aliveReward,
		]

		rewardWeight = [0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.1]
		sumber asal dari train_HumanoidBulletEnvHier-v0_41476_00000_0_2021-04-19_23-52-33
	update:
		Menyesuaikand dengan perubahan baru di low level
		high level hanya mengatur sudut target robot saja
		starting ep pos di re calculate tiap high level step
	global update:
	
	hasil: (00-21-49) Gagal mencari target, masih bisa jalan, kadang bisa mengejar, tapi overall gagal
	
train_HumanoidBulletEnvHier-v0_03457_00000_0_2021-04-19_23-57-59 (BASE)
train_HumanoidBulletEnvHier-v0_57cd8_00000_0_2021-04-20_00-21-49
train_HumanoidBulletEnvHier-v0_50898_00000_0_2021-04-20_07-59-44
train_HumanoidBulletEnvHier-v0_ad955_00000_0_2021-04-20_13-24-28:
	note:
		sumber asal dari train_HumanoidBulletEnvHier-v0_03457_00000_0_2021-04-19_23-57-59
	update:
		randomize yaw -180 s/d 180
		randomize target -180 s/d 180
		episode langsung done jika jarak robot dan target > targetLen+1
		syarat ganti target berubah dari dist <= 1 menjadi dist <= 0.5
		batas highLevelDegTarget berubah dari -45 s/d 45 menjadi -90 s/d 90
		(11-58-43) reward high level menjadi delta_highScore * step per level + cumulative alive reward, cumulative alive reward di reset setiap high level dapat reward
	global update:
	
	hasil:	(00-21-49) Gagal mencari target, masih bisa jalan, kadang bisa mengejar, tapi overall gagal
		(07-59-44) Terkadang high level memberi arah ke target, tapi lebih sering tidak
		(13-24-28) Low level sudah lumayan mengejar target, high level masih jelek
==> Kembali ke low level untuk di train ulang

train_HumanoidBulletEnvHier-v0_61e86_00000_0_2021-04-21_15-51-29 (BASE)
train_HumanoidBulletEnvHier-v0_906d4_00000_0_2021-04-21_15-52-47
train_HumanoidBulletEnvHier-v0_838c4_00000_0_2021-04-21_18-37-04:
	note:
		low level model dari PPO_HumanoidBulletEnvLow-v0_71287_00000_0_2021-04-21_08-35-16 iterasi 3700
	update:
		tambah body posture reward untuk low level
	global update:
	
	hasil:	(15-52-47) Lumayan tapi masih kurang
		(18-37-04) Terlalu lama train, hasil iterasi 1770 lebih jelek dari iterasi 970an
		
train_HumanoidBulletEnvHier-v0_61e86_00000_0_2021-04-21_15-51-29 (BASE)
train_HumanoidBulletEnvHier-v0_dd669_00000_0_2021-04-22_16-15-14:
	note:
		low level model dari PPO_HumanoidBulletEnvLow-v0_71287_00000_0_2021-04-21_08-35-16 iterasi 3700
		(16-15-14) hanya mentrain high level saja
	update:
		tambah drift score untuk high level, tujuannya agar robot tetap berada dalam jalur yang ditentukan
		(20-23-08) Tambah sudut robot ke starting_robot_pos kedalam observasi high level 
	global update:
	
	hasil:	(16-15-14) Lumayan bagus, drift tidak terlalu besar
	
train_HumanoidBulletEnvHier-v0_61e86_00000_0_2021-04-21_15-51-29 (BASE)
train_HumanoidBulletEnvHier-v0_7eecc_00000_0_2021-04-22_20-23-08:
	note:
		low level model dari PPO_HumanoidBulletEnvLow-v0_71287_00000_0_2021-04-21_08-35-16 iterasi 3700
		(20-23-08) Low level dan high level di train
		
	update:
		perhitungan drift score diperbaiki, sebelumnya starting robot pos mengacu ke posisi robot terakhir saat target berganti, seharusnya mengacu ke target sebelumnya
	global update:

	hasil:	(20-23-08) mengikuti target, drift besar, kurang stabil
	
train_HumanoidBulletEnvHier-v0_f8068_00000_0_2021-04-23_00-08-25 (BASE)
train_HumanoidBulletEnvHier-v0_49bc9_00000_0_2021-04-23_00-10-42:
	note:
		low level model dari PPO_HumanoidBulletEnvLow-v0_71287_00000_0_2021-04-21_08-35-16 iterasi 3700
		(00-10-42) Hanya high level yang di train
		
	update:
		
	global update:
		free log std high level diset ke True

	hasil:	(00-10-42) Lumayan bagus, jika target ada di sudut yang ekstrim robot akan jatuh / berputar dengan drift yang besar
	
train_HumanoidBulletEnvHier-v0_a56d7_00000_0_2021-04-26_13-02-48
train_HumanoidBulletEnvHier-v0_89e2b_00000_0_2021-04-26_15-39-31:
	note: 
		low level model 256, 128
		PPO_HumanoidBulletEnv-v0-Low_166df_00000_0_2021-04-25_19-33-42 checkpoint_2580 (BASE LOW)
		train_HumanoidBulletEnvHier-v0_a56d7_00000_0_2021-04-26_13-02-48 (BASE HIER)
	hasil: (15-39-51) Lumayan bagus, tidak terlalu ngedrift
	
train_HumanoidBulletEnvHier-v0_a56d7_00000_0_2021-04-26_13-02-48
train_HumanoidBulletEnvHier-v0_ddce5_00000_0_2021-04-27_20-12-42:
	note: 
		low level model 256, 128
		PPO_HumanoidBulletEnv-v0-Low_166df_00000_0_2021-04-25_19-33-42 checkpoint_2580 (BASE LOW)
		train_HumanoidBulletEnvHier-v0_a56d7_00000_0_2021-04-26_13-02-48 (BASE HIER)
	update: score drift dari exp(-3 * score) menjadi exp(-6 * score)
		weight drift score = 0.7, high target = 0.3
	hasil: (20-12-42) Lebih bagus daripada 15-39-51
	
train_HumanoidBulletEnvHier-v0_1ceff_00000_0_2021-04-28_09-14-43
train_HumanoidBulletEnvHier-v0_62ab1_00000_0_2021-04-28_09-16-40:
	note: 
		low level model 256, 128
		PPO_HumanoidBulletEnv-v0-Low_166df_00000_0_2021-04-25_19-33-42 checkpoint_2580 (BASE LOW)
		train_HumanoidBulletEnvHier-v0_1ceff_00000_0_2021-04-28_09-14-43 (BASE HIER)
	update: score drift dari exp(-3 * score) menjadi exp(-6 * score)
		weight drift score = 0.7, high target = 0.3
	global update:
		high level model 256,128 => 64, 64, 64
		lr 0.0005 => 0.00005
		sgd minibatch 12000 => 512
		train batch 36000 => 6000	
	hasil: (09-16-40) Kurang bagus

train_HumanoidBulletEnvHier-v0_8b503_00000_0_2021-04-28_14-25-36
train_HumanoidBulletEnvHier-v0_cf27e_00000_0_2021-04-28_14-27-30:
	note: 
		low level model 256, 128
		PPO_HumanoidBulletEnv-v0-Low_166df_00000_0_2021-04-25_19-33-42 checkpoint_2580 (BASE LOW)
		train_HumanoidBulletEnvHier-v0_8b503_00000_0_2021-04-28_14-25-36 (BASE HIER)
	update: weight drift score = 0.9, high target = 0.1
	global update:
		high level model 256,128
		lr 0.00005
		sgd minibatch 512
		train batch 6000	
	hasil: (14-27-30) Skip, ganti low level model
	
============================================
Low level model tidak menggunakan endpoint reward, hasil lebih bagus, lebih stabil dari sebelumnya
============================================
train_HumanoidBulletEnvHier-v0_0cbfa_00000_0_2021-05-01_17-03-09:
	note:
		PPO_HumanoidBulletEnv-v0-Low_6d114_00000_0_2021-04-30_23-26-25 checkpoint_1690(BASE LOW)
		train_HumanoidBulletEnvHier-v0_e1686_00000_0_2021-05-01_17-01-56 (BASE HIER)
	update: drift score = 0.9, high target = 0.1
	global update:
	
	hasil: Sangat bagus, drift minimal dan robot tidak jatuh
		
