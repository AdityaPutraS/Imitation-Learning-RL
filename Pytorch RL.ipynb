{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:44:23.527331Z",
     "start_time": "2021-02-01T03:44:22.732433Z"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import gym\n",
    "import gym.spaces\n",
    "import pybullet\n",
    "import pybullet_data\n",
    "import pybullet_envs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pfrl\n",
    "from pfrl import experiments, utils\n",
    "from pfrl.agents import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:44:23.577405Z",
     "start_time": "2021-02-01T03:44:23.534695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:44:23.651825Z",
     "start_time": "2021-02-01T03:44:23.597134Z"
    }
   },
   "outputs": [],
   "source": [
    "num_envs = 1\n",
    "seed = 13517013\n",
    "process_seeds = np.arange(num_envs) + seed * num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:44:24.553578Z",
     "start_time": "2021-02-01T03:44:24.550681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Buat environment\n",
    "def make_env(process_idx, test):\n",
    "    env = gym.make(\"HumanoidBulletEnv-v0\")\n",
    "    process_seed = int(process_seeds[process_idx])\n",
    "    env_seed = 2 ** 32 - 1 - process_seed if test else process_seed\n",
    "    env.seed(env_seed)\n",
    "    env = pfrl.wrappers.CastObservationToFloat32(env)\n",
    "    env = pfrl.wrappers.Monitor(env, \"out\", force=True)\n",
    "    env = pfrl.wrappers.Render(env, mode='human')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:44:26.872690Z",
     "start_time": "2021-02-01T03:44:26.870047Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_batch_env(test):\n",
    "    return pfrl.envs.MultiprocessVectorEnv(\n",
    "        [\n",
    "            functools.partial(make_env, idx, test)\n",
    "            for idx, env in enumerate(range(num_envs))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:44:27.272125Z",
     "start_time": "2021-02-01T03:44:27.173520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (44,), float32)\n",
      "Action space: Box(-1.0, 1.0, (17,), float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/miniconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "sample_env = gym.make(\"HumanoidBulletEnv-v0\")\n",
    "timestep_limit = sample_env.spec.max_episode_steps\n",
    "obs_space = sample_env.observation_space\n",
    "action_space = sample_env.action_space\n",
    "print(\"Observation space:\", obs_space)\n",
    "print(\"Action space:\", action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:44:27.582405Z",
     "start_time": "2021-02-01T03:44:27.576607Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_normalizer = pfrl.nn.EmpiricalNormalization(\n",
    "    obs_space.low.size, clip_threshold=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T03:44:29.391332Z",
     "start_time": "2021-02-01T03:44:29.389230Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_size = obs_space.low.size\n",
    "action_size = action_space.low.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:50.549035Z",
     "start_time": "2021-02-01T05:21:50.545043Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = torch.nn.Sequential(\n",
    "    nn.Linear(obs_size, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, action_size),\n",
    "    pfrl.policies.GaussianHeadWithStateIndependentCovariance(\n",
    "        action_size=action_size,\n",
    "        var_type=\"diagonal\",\n",
    "        var_func=lambda x: torch.exp(3 * x),  # Parameterize log std\n",
    "        var_param_init=0,  # log std = 0 => std = 1\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:50.775033Z",
     "start_time": "2021-02-01T05:21:50.771770Z"
    }
   },
   "outputs": [],
   "source": [
    "vf = torch.nn.Sequential(\n",
    "    nn.Linear(obs_size, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:50.963693Z",
     "start_time": "2021-02-01T05:21:50.960995Z"
    }
   },
   "outputs": [],
   "source": [
    "def ortho_init(layer, gain):\n",
    "    nn.init.orthogonal_(layer.weight, gain=gain)\n",
    "    nn.init.zeros_(layer.bias)\n",
    "\n",
    "# ortho_init(policy[0], gain=1)\n",
    "# ortho_init(policy[2], gain=1e-2)\n",
    "# ortho_init(vf[0], gain=1)\n",
    "# ortho_init(vf[2], gain=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:51.170808Z",
     "start_time": "2021-02-01T05:21:51.167952Z"
    }
   },
   "outputs": [],
   "source": [
    "model = pfrl.nn.Branched(policy, vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:51.384506Z",
     "start_time": "2021-02-01T05:21:51.381839Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.00015, eps=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:51.594288Z",
     "start_time": "2021-02-01T05:21:51.591845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "update_interval = 1024\n",
    "batch_size = 64\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:51.808657Z",
     "start_time": "2021-02-01T05:21:51.805070Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = PPO(\n",
    "        model,\n",
    "        opt,\n",
    "        obs_normalizer=obs_normalizer,\n",
    "        gpu=0,\n",
    "        update_interval=update_interval,\n",
    "        minibatch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        clip_eps_vf=None,\n",
    "        entropy_coef=0,\n",
    "        standardize_advantages=True,\n",
    "        gamma=0.995,\n",
    "        lambd=0.97,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:52.007652Z",
     "start_time": "2021-02-01T05:21:52.005589Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_n_runs = 100\n",
    "eval_interval = 100000\n",
    "log_interval = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:21:52.202857Z",
     "start_time": "2021-02-01T05:21:52.200590Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:39:10.554892Z",
     "start_time": "2021-02-01T05:21:52.389236Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/miniconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/aditya/miniconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:10000 episode:513 last_R: -87.18942574971152 average_R:-82.5369220793851\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -44.641235), ('average_entropy', 23.312328), ('average_value_loss', 145.27660858154297), ('average_policy_loss', -0.06800873281434178), ('n_updates', 2880), ('explained_variance', 0.6334418008167624)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:20000 episode:1017 last_R: -58.50459747700224 average_R:-62.19464056121581\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -32.487648), ('average_entropy', 21.924723), ('average_value_loss', 46.66379871368408), ('average_policy_loss', -0.11448954045772552), ('n_updates', 6080), ('explained_variance', 0.8331256613061919)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:30000 episode:1515 last_R: -48.97214558664273 average_R:-46.611620747349264\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -24.950844), ('average_entropy', 20.439636), ('average_value_loss', 20.200143022537233), ('average_policy_loss', -0.13116596765816213), ('n_updates', 9280), ('explained_variance', 0.8633622818623369)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:40000 episode:2008 last_R: -26.267748351542103 average_R:-35.724102834321464\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -18.148367), ('average_entropy', 18.768845), ('average_value_loss', 13.382986850738526), ('average_policy_loss', -0.11047462057322263), ('n_updates', 12480), ('explained_variance', 0.8172838327018304)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:50000 episode:2495 last_R: -32.72907224598166 average_R:-24.3983854787027\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -12.11634), ('average_entropy', 17.103333), ('average_value_loss', 8.282566385269165), ('average_policy_loss', -0.10845924649387598), ('n_updates', 15360), ('explained_variance', 0.7344986578631969)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:60000 episode:2975 last_R: -16.784110566577873 average_R:-15.189948621595995\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -7.2750072), ('average_entropy', 15.40245), ('average_value_loss', 6.351397273540496), ('average_policy_loss', -0.09579401385970414), ('n_updates', 18560), ('explained_variance', 0.7058851901616257)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:70000 episode:3445 last_R: -6.016327802215528 average_R:-10.512906015545669\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -5.288535), ('average_entropy', 14.141788), ('average_value_loss', 9.613599770069122), ('average_policy_loss', -0.08395534928888082), ('n_updates', 21760), ('explained_variance', 0.48605915177293346)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:80000 episode:3879 last_R: -1.494949725137848 average_R:-6.6269386139133575\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -3.2949538), ('average_entropy', 12.914232), ('average_value_loss', 5.620913956165314), ('average_policy_loss', -0.08544873802922666), ('n_updates', 24960), ('explained_variance', 0.38430679099793397)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:90000 episode:4301 last_R: -4.767974329243589 average_R:-3.2453998030702063\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -0.06197189), ('average_entropy', 11.723953), ('average_value_loss', 3.514206006526947), ('average_policy_loss', -0.10006651136092842), ('n_updates', 27840), ('explained_variance', 0.5308047706931011)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:100000 episode:4645 last_R: 13.670866791394655 average_R:0.8487581796682783\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 2.6127417), ('average_entropy', 10.883254), ('average_value_loss', 8.570575909614563), ('average_policy_loss', -0.0960652126185596), ('n_updates', 31040), ('explained_variance', 0.5892534357847237)]\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 28 R: -3.5607324530239577\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 35 R: 6.8610012074248505\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 26 R: 0.9827048555889641\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 35 R: 0.9148025838410834\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 53 R: -3.990291493112458\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 43 R: 0.26904314189596024\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 31 R: -0.9188408212299697\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 33 R: -0.23978327685035827\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 28 R: -5.744648131496795\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 25 R: -21.385666654005757\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 37 R: 12.278406253436692\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 21 R: -6.432208026916487\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 37 R: -11.130554941597808\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 26 R: -0.6888154945612768\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 20 R: -2.4184063478649476\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 44 R: -15.043823997031723\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 28 R: 10.053258403881044\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 41 R: -5.814557870480351\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 38 R: 14.805215996230256\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 34 R: 12.45452570369816\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 34 R: 7.790907805990717\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 27 R: -10.594399745878764\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 55 R: 0.4900436697833319\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 36 R: 9.780405569061985\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 49 R: -7.233912900942963\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 31 R: -17.09448906635371\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 24 R: 5.464601607392251\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 33 R: 4.188389507609826\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 30 R: 5.908683653146726\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 42 R: 8.985759500440325\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 30 length: 29 R: 4.435538721091869\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 31 length: 45 R: 23.135410329503067\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 32 length: 35 R: 4.723660521415877\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 33 length: 59 R: 7.434302597219355\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 34 length: 21 R: -12.578004904123375\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 35 length: 52 R: -47.296802013677365\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 36 length: 32 R: -8.07064391880849\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 37 length: 40 R: 2.1126149865667676\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 38 length: 36 R: 12.837247928227583\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 39 length: 38 R: 13.83462894285621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 40 length: 36 R: -25.466273990763874\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 41 length: 26 R: 2.638522980826382\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 42 length: 39 R: 8.883444561160285\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 43 length: 35 R: 7.674759586533764\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 44 length: 29 R: 2.8502010728814633\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 45 length: 29 R: 4.107766388692833\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 46 length: 28 R: 8.066941942776609\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 47 length: 35 R: -8.286732186161682\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 48 length: 38 R: 4.519144858936488\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 49 length: 27 R: -0.08731545544287744\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 50 length: 26 R: -2.5507431003672547\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 51 length: 53 R: 7.609655237414698\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 52 length: 25 R: 4.8133067803020815\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 53 length: 48 R: -8.437652896376678\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 54 length: 33 R: -7.879142865163157\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 55 length: 30 R: 3.550474461163685\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 56 length: 27 R: -1.923156721676059\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 57 length: 37 R: 12.36149806678586\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 58 length: 31 R: -5.923828440958459\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 59 length: 33 R: 9.579323662800014\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 60 length: 33 R: -9.196916477067862\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 61 length: 37 R: 5.907154627599809\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 62 length: 25 R: -9.061689357107392\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 63 length: 50 R: -29.63117590519978\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 64 length: 30 R: 9.201153468797564\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 65 length: 44 R: 8.279394178345681\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 66 length: 41 R: 17.3895239862657\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 67 length: 37 R: -4.5922820679363205\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 68 length: 32 R: 3.4242130961996717\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 69 length: 34 R: 4.881122159129883\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 70 length: 51 R: 30.118130095639202\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 71 length: 29 R: -19.40963137668004\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 72 length: 32 R: -1.67292274676438\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 73 length: 30 R: -0.027469164867945484\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 74 length: 38 R: 8.058202583891397\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 75 length: 51 R: -6.895636388957796\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 76 length: 73 R: -75.17051345100627\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 77 length: 38 R: 3.8198893565233427\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 78 length: 48 R: -0.9195099977252537\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 79 length: 49 R: 0.5556042268566657\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 80 length: 65 R: 24.807966374419635\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 81 length: 27 R: -0.02850766569317864\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 82 length: 47 R: 5.668736425672249\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 83 length: 52 R: -6.539866397157313\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 84 length: 22 R: -10.176568749123543\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 85 length: 38 R: 1.3278835805569527\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 86 length: 38 R: 6.286014268455619\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 87 length: 31 R: 10.031304918348905\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 88 length: 36 R: 0.3211323851792258\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 89 length: 39 R: 21.123624992147956\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 90 length: 40 R: 9.8174693041743\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 91 length: 43 R: 1.751734039724397\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 92 length: 32 R: 3.1914262026752116\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 93 length: 37 R: -6.839591549175386\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 94 length: 28 R: -1.2011802608205469\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 95 length: 41 R: -11.606940498141924\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 96 length: 37 R: 1.1763420917035559\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 97 length: 23 R: -21.62865473274869\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 98 length: 34 R: 5.450756108862696\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 99 length: 26 R: -2.0649557668075436\n",
      "INFO:pfrl.experiments.train_agent_batch:The best score is updated -3.4028235e+38 -> -0.28470468710101077\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:110000 episode:4884 last_R: 9.129509440340913 average_R:2.553824480138688\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 7.723677), ('average_entropy', 10.2986965), ('average_value_loss', 15.000926094055176), ('average_policy_loss', -0.0967065498419106), ('n_updates', 34240), ('explained_variance', 0.6014549681387453)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:120000 episode:5104 last_R: -6.36669576901186 average_R:6.947913160151874\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 10.953047), ('average_entropy', 9.741592), ('average_value_loss', 15.602313957214356), ('average_policy_loss', -0.09763376727700233), ('n_updates', 37440), ('explained_variance', 0.7347873933792843)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:130000 episode:5303 last_R: 24.003463281430594 average_R:12.428924878021265\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 14.35525), ('average_entropy', 9.100825), ('average_value_loss', 23.648781118392943), ('average_policy_loss', -0.08588817620649934), ('n_updates', 40320), ('explained_variance', 0.7026327573109157)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:140000 episode:5499 last_R: 34.84074885167065 average_R:11.717893502436123\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 16.853374), ('average_entropy', 8.479092), ('average_value_loss', 13.9325519323349), ('average_policy_loss', -0.07001186208799481), ('n_updates', 43520), ('explained_variance', 0.8240723185049069)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:150000 episode:5694 last_R: 33.97942620187968 average_R:15.908057246607479\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 16.385424), ('average_entropy', 8.083976), ('average_value_loss', 25.110380268096925), ('average_policy_loss', -0.07845405244268477), ('n_updates', 46720), ('explained_variance', 0.8204357465756443)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:160000 episode:5883 last_R: 25.68329448641598 average_R:18.331920281575147\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 18.864895), ('average_entropy', 7.5501766), ('average_value_loss', 35.4779026889801), ('average_policy_loss', -0.07381778312847018), ('n_updates', 49920), ('explained_variance', 0.8263797240930723)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:170000 episode:6064 last_R: 8.56794165695464 average_R:20.25621294163793\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 14.0261), ('average_entropy', 6.9025793), ('average_value_loss', 45.46721496582031), ('average_policy_loss', -0.08432886777445674), ('n_updates', 53120), ('explained_variance', 0.7984155570314895)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:180000 episode:6246 last_R: 5.32391042042145 average_R:16.279872313079057\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 23.716734), ('average_entropy', 6.4508533), ('average_value_loss', 22.135118579864503), ('average_policy_loss', -0.09200455007143318), ('n_updates', 56000), ('explained_variance', 0.7542905209632937)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:190000 episode:6421 last_R: 34.528274312586284 average_R:27.818822283441282\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 22.513332), ('average_entropy', 6.323576), ('average_value_loss', 24.688920001983643), ('average_policy_loss', -0.07675195874646307), ('n_updates', 59200), ('explained_variance', 0.8000555783336739)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:200000 episode:6598 last_R: -22.62504744643228 average_R:23.548642940875485\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 21.740831), ('average_entropy', 5.917349), ('average_value_loss', 16.367240772247314), ('average_policy_loss', -0.06976678734645247), ('n_updates', 62400), ('explained_variance', 0.8731378587746509)]\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 0 length: 54 R: 50.48447010454692\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 1 length: 43 R: -56.15353635151405\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 2 length: 52 R: 43.616335693094875\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 3 length: 58 R: 25.352141175595165\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 4 length: 65 R: 41.91779672938139\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 5 length: 50 R: 34.773283005140556\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 6 length: 54 R: 41.591147832939164\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 7 length: 58 R: 44.970332722995956\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 8 length: 60 R: 44.8217970604368\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 9 length: 53 R: 45.77943032143522\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 10 length: 47 R: 35.54307299212232\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 11 length: 58 R: 15.593299288005802\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 12 length: 57 R: -20.24306813101576\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 13 length: 70 R: 44.85560709526325\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 14 length: 60 R: 37.13385222512587\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 15 length: 60 R: 36.81724585746123\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 16 length: 49 R: 49.86220047616952\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 17 length: 52 R: 50.35962916607968\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 18 length: 54 R: 39.41424654898874\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 19 length: 81 R: 31.533614802597725\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 20 length: 84 R: 59.251209317699264\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 21 length: 50 R: 30.858371329207145\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 22 length: 111 R: -57.9258057510655\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 23 length: 49 R: 42.07465925603755\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 24 length: 50 R: 37.289896380092244\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 25 length: 58 R: 46.79355870869767\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 26 length: 65 R: -93.33654397400096\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 27 length: 61 R: 44.66843614799145\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 28 length: 57 R: 50.70937337665527\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 29 length: 47 R: 19.329507005756014\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 30 length: 76 R: 46.262183542089765\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 31 length: 86 R: 25.28400826740252\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 32 length: 47 R: 32.01259564827342\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 33 length: 49 R: 37.589344822416024\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 34 length: 56 R: 45.19621083735837\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 35 length: 67 R: 58.36917948052867\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 36 length: 44 R: 34.69742148300138\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 37 length: 55 R: 38.4556479506602\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 38 length: 51 R: 39.749443668755696\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 39 length: 45 R: 36.621741633139024\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 40 length: 60 R: 34.35529277368187\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 41 length: 61 R: 19.779327954507608\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 42 length: 48 R: 39.935180940105056\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 43 length: 47 R: 17.279122181049026\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 44 length: 44 R: 39.31180411881941\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 45 length: 137 R: -95.3479961387915\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 46 length: 61 R: 32.4222388262875\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 47 length: 47 R: -19.478079095878634\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 48 length: 45 R: -4.903257526129893\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 49 length: 51 R: 35.84363953876454\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 50 length: 48 R: -47.49587533599842\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 51 length: 53 R: 42.849714249144014\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 52 length: 68 R: 33.63187483049696\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 53 length: 60 R: 21.524106136681798\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 54 length: 71 R: 49.84303029668807\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 55 length: 44 R: 29.432861359261732\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 56 length: 47 R: 31.075680160350746\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 57 length: 89 R: 1.0470961343758876\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 58 length: 59 R: 54.17436659215017\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 59 length: 71 R: -47.31203119499984\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 60 length: 50 R: 44.27833806100388\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 61 length: 56 R: 34.624319708753326\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 62 length: 47 R: 27.194232407683735\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 63 length: 54 R: 49.74278448212425\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 64 length: 51 R: 41.3695874895915\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 65 length: 48 R: 38.21974092143791\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 66 length: 44 R: 35.50721561689716\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 67 length: 43 R: 35.41872315782676\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 68 length: 48 R: -15.935605247075733\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 69 length: 49 R: 26.374018347749374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 70 length: 72 R: 4.5593619635866975\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 71 length: 55 R: 41.997298946231595\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 72 length: 71 R: 34.14287819067977\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 73 length: 50 R: 40.877447503154684\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 74 length: 74 R: 43.41803625414178\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 75 length: 51 R: -73.67906335109872\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 76 length: 72 R: 51.918179270163826\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 77 length: 45 R: 39.15188146285655\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 78 length: 67 R: -4.357026763842445\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 79 length: 47 R: 42.87332550546998\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 80 length: 54 R: 30.695970385121473\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 81 length: 51 R: 32.79067770790862\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 82 length: 62 R: 41.20797053348506\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 83 length: 61 R: 39.58688303741072\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 84 length: 49 R: 24.492736768812755\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 85 length: 79 R: -54.78988051193735\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 86 length: 48 R: 37.69483750227519\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 87 length: 46 R: -14.220539859900612\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 88 length: 50 R: 37.56944876779454\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 89 length: 43 R: -16.190203695667158\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 90 length: 90 R: 33.7773345461217\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 91 length: 63 R: 38.15781782244449\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 92 length: 58 R: 37.583541571650144\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 93 length: 60 R: 32.8278776039966\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 94 length: 52 R: -3.5921170952395154\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 95 length: 70 R: -126.78998285753768\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 96 length: 52 R: -6.101290104132205\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 97 length: 44 R: 25.93365216173115\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 98 length: 49 R: 25.354801053834663\n",
      "INFO:pfrl.experiments.train_agent_batch:evaluation episode 99 length: 47 R: 39.96217025155345\n",
      "INFO:pfrl.experiments.train_agent_batch:The best score is updated -0.28470468710101077 -> 22.735888140631737\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:210000 episode:6773 last_R: 33.44214103007834 average_R:23.97911809627879\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 16.829243), ('average_entropy', 5.55986), ('average_value_loss', 42.24318196296692), ('average_policy_loss', -0.07265004875138402), ('n_updates', 65600), ('explained_variance', 0.8510755944228356)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:220000 episode:6962 last_R: -8.667543556395689 average_R:-8.970337687700887\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -0.8528393), ('average_entropy', 5.3496633), ('average_value_loss', 21.381179599761964), ('average_policy_loss', -0.098403360825032), ('n_updates', 68480), ('explained_variance', 0.7483322899498182)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:230000 episode:7156 last_R: 11.066016253254201 average_R:-1.0458910292106915\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 2.9792233), ('average_entropy', 4.992922), ('average_value_loss', 17.939403553009033), ('average_policy_loss', -0.09080860067158937), ('n_updates', 71680), ('explained_variance', 0.6765682713992851)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:240000 episode:7355 last_R: 23.929661912043226 average_R:-1.451029136763292\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 9.583465), ('average_entropy', 4.697016), ('average_value_loss', 18.776073141098024), ('average_policy_loss', -0.0112082823459059), ('n_updates', 74880), ('explained_variance', 0.8019812671983582)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:250000 episode:7561 last_R: -24.449132447192106 average_R:6.6217236001567485\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 12.866094), ('average_entropy', 4.5235147), ('average_value_loss', 36.57161095619202), ('average_policy_loss', 0.05960844490677118), ('n_updates', 78080), ('explained_variance', 0.8406356481169288)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:260000 episode:7793 last_R: 3.8371030391383085 average_R:-25.81739664414866\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -4.683867), ('average_entropy', 4.3113008), ('average_value_loss', 98.6571106338501), ('average_policy_loss', -0.07606196852400898), ('n_updates', 80960), ('explained_variance', 0.6057417587393397)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:270000 episode:8027 last_R: 4.970572782291853 average_R:-7.844417881995371\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', 3.760811), ('average_entropy', 4.029713), ('average_value_loss', 14.099493360519409), ('average_policy_loss', -0.08351209308952093), ('n_updates', 84160), ('explained_variance', 0.6043067151210215)]\n",
      "INFO:pfrl.experiments.train_agent_batch:outdir:out step:280000 episode:8253 last_R: -14.685346512662363 average_R:-13.278615868427881\n",
      "INFO:pfrl.experiments.train_agent_batch:statistics: [('average_value', -1.6394719), ('average_entropy', 3.8348258), ('average_value_loss', 152.5630182647705), ('average_policy_loss', -0.04489120675250888), ('n_updates', 87360), ('explained_variance', 0.2837426018631145)]\n",
      "INFO:pfrl.experiments.train_agent_batch:Saved the agent to out/285986_except\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-d2562099a5d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m experiments.train_agent_batch_with_evaluation(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_batch_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0meval_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_batch_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pfrl/experiments/train_agent_batch.py\u001b[0m in \u001b[0;36mtrain_agent_batch_with_evaluation\u001b[0;34m(agent, env, steps, eval_n_steps, eval_n_episodes, eval_interval, outdir, checkpoint_freq, max_episode_len, step_offset, eval_max_episode_len, return_window_size, eval_env, log_interval, successful_score, step_hooks, evaluation_hooks, save_best_so_far_agent, logger)\u001b[0m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     eval_stats_history = train_agent_batch(\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pfrl/experiments/train_agent_batch.py\u001b[0m in \u001b[0;36mtrain_agent_batch\u001b[0;34m(agent, env, steps, outdir, checkpoint_freq, log_interval, max_episode_len, step_offset, evaluator, successful_score, step_hooks, evaluation_hooks, return_window_size, logger)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# a_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;31m# o_{t+1}, r_{t+1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mobss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pfrl/agents/ppo.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, batch_obs)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_act_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_act_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pfrl/agents/ppo.py\u001b[0m in \u001b[0;36m_batch_act_train\u001b[0;34m(self, batch_obs)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 )\n\u001b[1;32m    710\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                 \u001b[0maction_distrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m             \u001b[0mbatch_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_distrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_distrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pfrl/nn/branched.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchild\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pfrl/nn/branched.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchild\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiments.train_agent_batch_with_evaluation(\n",
    "    agent=agent,\n",
    "    env=make_batch_env(False),\n",
    "    eval_env=make_batch_env(True),\n",
    "    outdir=\"out\",\n",
    "    steps=2000000,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=eval_n_runs,\n",
    "    eval_interval=eval_interval,\n",
    "    log_interval=log_interval,\n",
    "    max_episode_len=timestep_limit,\n",
    "    save_best_so_far_agent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
